# Raptor Traffic Recording/Replay: demo-service

## Overview

Raptor enables traffic recording and replay for demo-service. This allows you to:
- ğŸ“¹ Record real production/staging traffic
- â–¶ï¸ Replay traffic against test environments
- ğŸ” Detect API regressions automatically
- ğŸ“Š Generate test reports

## Quick Start

### Recording Traffic

```bash
# Start the recording proxy
./raptor/record.sh

# Send traffic to http://localhost:9080
# Proxy forwards to your service at http://localhost:8080

# Stop recording
Ctrl+C or ./raptor/stop-recording.sh
```

### Replaying Traffic

```bash
# Replay the latest recording
./raptor/replay.sh

# Replay a specific recording
./raptor/replay.sh raptor/recordings/my-recording.json
```

## Configuration Files

| File | Purpose |
|------|---------|
| `raptor/raptor.yml` | Main recording configuration |
| `raptor/replay.yml` | Replay and validation settings |
| `raptor/filters/default-filters.yml` | Header/body filters |
| `raptor/filters/sensitive-data.yml` | PII/sensitive data masking |

## Directory Structure

```
raptor/
â”œâ”€â”€ raptor.yml              # Recording config
â”œâ”€â”€ replay.yml              # Replay config
â”œâ”€â”€ record.sh               # Start recording script
â”œâ”€â”€ replay.sh               # Run replay script
â”œâ”€â”€ recordings/             # Recorded traffic files
â”‚   â””â”€â”€ sample-recording.json
â”œâ”€â”€ filters/                # Filter configurations
â”‚   â”œâ”€â”€ default-filters.yml
â”‚   â””â”€â”€ sensitive-data.yml
â””â”€â”€ reports/                # Test reports (generated)
```

## Recording Configuration

### What Gets Recorded

- âœ… HTTP method, path, query params
- âœ… Request/response headers
- âœ… Request/response bodies
- âœ… Response status codes
- âœ… Timing information

### What Gets Filtered

- ğŸ”’ Authorization headers (masked)
- ğŸ”’ Cookies (excluded)
- ğŸ”’ Passwords in body (masked)
- ğŸ”’ Credit card numbers (masked)
- ğŸ”’ JWT tokens (masked)

## Replay Validation

Raptor validates replayed responses against recorded responses:

| Check | Mode | Description |
|-------|------|-------------|
| Status Code | Exact/Range | HTTP status must match |
| Response Body | Semantic | JSON structure comparison |
| Response Time | Threshold | Must be under limit |

### Ignored Fields

These fields are ignored during comparison (configurable):
- `timestamp`
- `requestId`
- `createdAt`
- `updatedAt`

## Docker Usage

```bash
# Start recording proxy
docker-compose -f docker-compose.raptor.yml up raptor-proxy

# Run replay
docker-compose -f docker-compose.raptor.yml --profile replay up raptor-replay

# Full stack (with your app)
docker-compose -f docker-compose.raptor.yml --profile with-app up
```

## CI/CD Integration

The generated GitHub Actions workflow (`.github/workflows/raptor-tests.yml`) will:

1. Start your service
2. Replay recorded traffic
3. Validate responses
4. Generate JUnit reports
5. Fail build on regressions

## Best Practices

### Recording

1. **Record in staging** - Use realistic data
2. **Filter sensitive data** - Never record passwords/tokens
3. **Name recordings** - Use descriptive names
4. **Version recordings** - Track with git or artifacts

### Replay

1. **Reset state first** - Ensure clean environment
2. **Use hermetic server** - Predictable responses
3. **Ignore dynamic fields** - timestamps, IDs
4. **Set reasonable thresholds** - Account for variance

## Integration with Hermetic

Raptor works best with Hermetic servers:

```bash
# 1. Start hermetic server
docker-compose -f docker-compose.hermetic.yml up -d

# 2. Record against hermetic
TARGET_PORT=8080 ./raptor/record.sh

# 3. Replay against hermetic
TARGET_PORT=8080 ./raptor/replay.sh
```

## Generated by TARS
Test Automation and Review System
